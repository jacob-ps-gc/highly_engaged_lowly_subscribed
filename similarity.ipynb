{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77c20a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscriber Lookalike Analysis (calibrated propensity + optional similarity)\n",
    "\n",
    "# load libraries and turn off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, roc_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# set seed\n",
    "random.seed(42)\n",
    "\n",
    "# load data\n",
    "query_2 = pd.read_csv('df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53b608ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>AS_OF_DATE</th>\n",
       "      <th>CREATED_DATE</th>\n",
       "      <th>DAYS_SINCE_CREATED</th>\n",
       "      <th>IS_ACTIVE</th>\n",
       "      <th>DAYS_SINCE_LAST_ACTIVE</th>\n",
       "      <th>IS_IN_FREE_TRIAL_NOW</th>\n",
       "      <th>IS_PAYING_SUBSCRIBER_NOW</th>\n",
       "      <th>IS_SUBSCRIBER_NOW</th>\n",
       "      <th>MONTHLY_SUBSCRIBER_STATUS</th>\n",
       "      <th>...</th>\n",
       "      <th>FIRST_TRIAL_TS</th>\n",
       "      <th>HAS_TRIALED</th>\n",
       "      <th>N_SHARED_PLANS_OWNED</th>\n",
       "      <th>SEATS_NOW_TOTAL</th>\n",
       "      <th>NON_OWNER_SEATS_NOW_TOTAL</th>\n",
       "      <th>MEMBERS_WITH_PAID_SUB_NOW_TOTAL</th>\n",
       "      <th>FIRST_SHARED_PLAN_TS</th>\n",
       "      <th>CURRENT_PROCESSOR</th>\n",
       "      <th>CURRENT_PRICING_GROUP</th>\n",
       "      <th>CURRENT_GC_APP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>359ea7fc-1092-4acb-beb9-a69b37394ac0</td>\n",
       "      <td>2025-08-28</td>\n",
       "      <td>2023-08-16</td>\n",
       "      <td>743</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>inactive</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d91ec3f7-0c3e-49e7-81d8-e13bd11ac7a4</td>\n",
       "      <td>2025-08-28</td>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>1797</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>retained</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-07-04 00:41:45.670</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>google</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Team Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9a2d9219-5fc0-4700-944b-7fcdf20251bb</td>\n",
       "      <td>2025-08-28</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>891</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>inactive</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>247e0027-3392-4024-bfb7-14de6b710b83</td>\n",
       "      <td>2025-08-28</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>1914</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>inactive</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e64ad19b-5275-49bf-8eaa-a0d319b21e47</td>\n",
       "      <td>2025-08-28</td>\n",
       "      <td>2019-06-22</td>\n",
       "      <td>2259</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>retained</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-02-12 18:53:19.000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apple</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Team Manager</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                USER_ID  AS_OF_DATE CREATED_DATE  \\\n",
       "0  359ea7fc-1092-4acb-beb9-a69b37394ac0  2025-08-28   2023-08-16   \n",
       "1  d91ec3f7-0c3e-49e7-81d8-e13bd11ac7a4  2025-08-28   2020-09-26   \n",
       "2  9a2d9219-5fc0-4700-944b-7fcdf20251bb  2025-08-28   2023-03-21   \n",
       "3  247e0027-3392-4024-bfb7-14de6b710b83  2025-08-28   2020-06-01   \n",
       "4  e64ad19b-5275-49bf-8eaa-a0d319b21e47  2025-08-28   2019-06-22   \n",
       "\n",
       "   DAYS_SINCE_CREATED  IS_ACTIVE  DAYS_SINCE_LAST_ACTIVE  \\\n",
       "0                 743       True                       1   \n",
       "1                1797       True                       1   \n",
       "2                 891      False                       1   \n",
       "3                1914      False                       1   \n",
       "4                2259      False                      19   \n",
       "\n",
       "   IS_IN_FREE_TRIAL_NOW  IS_PAYING_SUBSCRIBER_NOW  IS_SUBSCRIBER_NOW  \\\n",
       "0                 False                     False              False   \n",
       "1                 False                      True               True   \n",
       "2                 False                     False              False   \n",
       "3                 False                     False              False   \n",
       "4                 False                      True               True   \n",
       "\n",
       "  MONTHLY_SUBSCRIBER_STATUS  ...           FIRST_TRIAL_TS HAS_TRIALED  \\\n",
       "0                  inactive  ...                      NaN       False   \n",
       "1                  retained  ...  2022-07-04 00:41:45.670        True   \n",
       "2                  inactive  ...                      NaN       False   \n",
       "3                  inactive  ...                      NaN       False   \n",
       "4                  retained  ...  2022-02-12 18:53:19.000        True   \n",
       "\n",
       "  N_SHARED_PLANS_OWNED SEATS_NOW_TOTAL NON_OWNER_SEATS_NOW_TOTAL  \\\n",
       "0                  NaN             NaN                       NaN   \n",
       "1                  NaN             NaN                       NaN   \n",
       "2                  NaN             NaN                       NaN   \n",
       "3                  NaN             NaN                       NaN   \n",
       "4                  NaN             NaN                       NaN   \n",
       "\n",
       "   MEMBERS_WITH_PAID_SUB_NOW_TOTAL  FIRST_SHARED_PLAN_TS  CURRENT_PROCESSOR  \\\n",
       "0                              NaN                   NaN                NaN   \n",
       "1                              NaN                   NaN             google   \n",
       "2                              NaN                   NaN                NaN   \n",
       "3                              NaN                   NaN                NaN   \n",
       "4                              NaN                   NaN              apple   \n",
       "\n",
       "   CURRENT_PRICING_GROUP  CURRENT_GC_APP  \n",
       "0                    NaN             NaN  \n",
       "1                 2022.0    Team Manager  \n",
       "2                    NaN             NaN  \n",
       "3                    NaN             NaN  \n",
       "4                 2025.0    Team Manager  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e1fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up our functions\n",
    "\n",
    "# the goal is to combine random forest classification with logistic regression so that we get the best of both methods\n",
    "# we're going to create a few functions here to define and clean\n",
    "class SubscriberLookalikeAnalysis:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.features = None\n",
    "        self.target = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.rf_model = None          # calibrated RF\n",
    "        self.lr_model = None          # calibrated LR\n",
    "        self.rf_aux_for_importance = None\n",
    "        self.feature_importance = None\n",
    "        self.eval_metrics = {}\n",
    "        self.lift_df = None\n",
    "        self.test_index_mask = None   # indices for test split\n",
    "        self.ensemble_probs_all = None\n",
    "\n",
    "    # set up a feature engineering function and add a lil tracker so we know where we are in the process\n",
    "    def prepare_features(self):\n",
    "      print(\"Starting feature engineering...\")\n",
    "\n",
    "      self.df['is_subscriber_target'] = self.df['is_subscriber_now'].astype(int)\n",
    "\n",
    "      # ratios, recency scores\n",
    "      self.df['engagement_per_team'] = self.df['n_teams_interacted'] / (self.df['n_teams'] + 1)\n",
    "      self.df['content_per_team'] = (\n",
    "            self.df['n_livestreams_viewed']\n",
    "          + self.df['n_archived_videos_viewed']\n",
    "      ) / (self.df['n_teams'] + 1)\n",
    "      dsl = self.df['days_since_last_active'].fillna(999)\n",
    "      self.df['activity_recency_score'] = np.select(\n",
    "          condlist   = [dsl.eq(0), dsl.le(7), dsl.le(30)]\n",
    "        , choicelist = [10, 5, 2]\n",
    "        , default    = 1\n",
    "      )\n",
    "\n",
    "      # engagement intensity\n",
    "      self.df['total_content_viewed'] = (\n",
    "            self.df['n_livestreams_viewed']\n",
    "          + self.df['n_archived_videos_viewed']\n",
    "          + self.df['n_film_review_videos_viewed']\n",
    "      )\n",
    "      self.df['total_stats_viewed'] = (\n",
    "            self.df['n_play_by_plays_viewed']\n",
    "          + self.df['n_player_stats_viewed']\n",
    "          + self.df['n_box_scores_viewed']\n",
    "      )\n",
    "\n",
    "      # team roles depth\n",
    "      self.df['total_roles'] = (\n",
    "            self.df['n_staff_roles']\n",
    "          + self.df['n_family_roles']\n",
    "          + self.df['n_fan_roles']\n",
    "          + self.df['n_player_roles']\n",
    "      )\n",
    "      self.df['current_roles'] = (\n",
    "            self.df['n_current_staff_roles']\n",
    "          + self.df['n_current_family_roles']\n",
    "          + self.df['n_current_fan_roles']\n",
    "          + self.df['n_current_player_roles']\n",
    "      )\n",
    "\n",
    "      # account maturity\n",
    "      self.df['account_age_weeks'] = self.df['days_since_created'] / 7.0\n",
    "      self.df['is_new_user'] = (self.df['days_since_created'] <= 30).astype(int)\n",
    "\n",
    "      # coverage producer rollup\n",
    "      self.df['is_any_coverage_producer'] = (\n",
    "            self.df['is_successful_coverage_producer'].astype(int)\n",
    "          | self.df['is_successful_scoring_coverage_producer'].astype(int)\n",
    "          | self.df['is_successful_streaming_coverage_producer'].astype(int)\n",
    "      ).astype(int)\n",
    "\n",
    "      feature_columns = [\n",
    "          # engagement flags\n",
    "          'is_active','is_engaged','is_super_engaged'\n",
    "        , 'is_team_management_engaged','is_team_management_super_engaged'\n",
    "        , 'is_free_feature_engaged','is_free_feature_super_engaged'\n",
    "        , 'is_paid_feature_engaged','is_paid_feature_super_engaged'\n",
    "        , 'is_coverage_producer_engaged','is_coverage_producer_super_engaged'\n",
    "          # content\n",
    "        , 'n_livestreams_viewed','n_archived_videos_viewed','n_film_review_videos_viewed'\n",
    "        , 'total_content_viewed','n_seconds_livestreams_viewed','n_seconds_archived_videos_viewed'\n",
    "          # stats\n",
    "        , 'n_play_by_plays_viewed','n_player_stats_viewed','n_box_scores_viewed'\n",
    "        , 'n_spray_charts_viewed','n_animated_gamestreams_viewed','total_stats_viewed'\n",
    "          # teams & roles\n",
    "        , 'n_teams','n_current_teams','n_teams_interacted','total_roles','current_roles'\n",
    "        , 'n_rec_teams','n_club_teams','n_school_teams'\n",
    "          # activity\n",
    "        , 'n_games_streamed_60_days','n_games_scored_60_days','n_games_covered_60_days'\n",
    "        , 'n_messages_sent','n_events_created','n_events_rsvpd','n_users_invited'\n",
    "          # coverage\n",
    "        , 'is_any_coverage_producer'\n",
    "          # account\n",
    "        , 'days_since_created','account_age_weeks','is_new_user'\n",
    "        , 'activity_recency_score','engagement_per_team','content_per_team'\n",
    "          # trial\n",
    "        , 'has_trialed'\n",
    "      ]\n",
    "\n",
    "      # one-hots: sport\n",
    "      if 'primary_sport_daily' in self.df.columns:\n",
    "          sport_dummies = pd.get_dummies(\n",
    "                self.df['primary_sport_daily']\n",
    "              , prefix    = 'sport'\n",
    "              , dummy_na  = True\n",
    "          )\n",
    "          self.df = pd.concat([self.df, sport_dummies], axis = 1)\n",
    "          feature_columns.extend(sport_dummies.columns.tolist())\n",
    "\n",
    "      # one-hots: competition level (DAILY)\n",
    "      if 'primary_competition_daily' in self.df.columns:\n",
    "          comp_dummies = pd.get_dummies(\n",
    "                self.df['primary_competition_daily']\n",
    "              , prefix    = 'comp'\n",
    "              , dummy_na  = True\n",
    "          )\n",
    "          self.df = pd.concat([self.df, comp_dummies], axis = 1)\n",
    "          feature_columns.extend(comp_dummies.columns.tolist())\n",
    "\n",
    "      # one-hots: age group (SELF)\n",
    "      if 'self_age_group_condensed' in self.df.columns:\n",
    "          age_dummies = pd.get_dummies(\n",
    "                self.df['self_age_group_condensed']\n",
    "              , prefix    = 'age'\n",
    "              , dummy_na  = True\n",
    "          )\n",
    "          self.df = pd.concat([self.df, age_dummies], axis = 1)\n",
    "          feature_columns.extend(age_dummies.columns.tolist())\n",
    "\n",
    "      # top-10 states as binary flags\n",
    "      if 'primary_state' in self.df.columns:\n",
    "          top_states = self.df['primary_state'].value_counts().head(10).index\n",
    "          for st in top_states:\n",
    "              col = f'state_{st}'\n",
    "              self.df[col] = (self.df['primary_state'] == st).astype(int)\n",
    "              feature_columns.append(col)\n",
    "\n",
    "      available = [c for c in feature_columns if c in self.df.columns]\n",
    "      self.features = self.df[available].fillna(0)\n",
    "      self.target   = self.df['is_subscriber_target']\n",
    "\n",
    "      print(f\"Feature engineering complete. Using {len(available)} features.\")\n",
    "      return self.features , self.target\n",
    "\n",
    "\n",
    "    # lift table for assessment\n",
    "    @staticmethod\n",
    "    def lift_table(scores, y, bins = 10):\n",
    "        df = pd.DataFrame({'s': scores, 'y': y}).sort_values('s', ascending = False)\n",
    "        df['decile'] = pd.qcut(df['s'], bins, labels = False, duplicates = 'drop')\n",
    "        agg = df.groupby('decile').agg(\n",
    "              pos_rate = ('y','mean')\n",
    "            , count    = ('y','size')\n",
    "        ).sort_index(ascending = False).reset_index(drop = True)\n",
    "        overall = df['y'].mean()\n",
    "        agg['lift'] = agg['pos_rate'] / (overall + 1e-9)\n",
    "        return agg , overall\n",
    "\n",
    "    # training!\n",
    "    def train_propensity_models(\n",
    "        self\n",
    "      , test_size = 0.2\n",
    "      , random_state = 42\n",
    "      , cutoff_date = None        # e.g., '2025-06-30' for time-based split\n",
    "    ):\n",
    "        print(\"Training propensity models (calibrated)...\")\n",
    "\n",
    "        X = self.features\n",
    "        y = self.target.values\n",
    "\n",
    "        if cutoff_date is not None and 'as_of_date' in self.df.columns:\n",
    "            cutoff = pd.to_datetime(cutoff_date)\n",
    "            mask_tr = pd.to_datetime(self.df['as_of_date']) <= cutoff\n",
    "            X_train , X_test = X[mask_tr] , X[~mask_tr]\n",
    "            y_train , y_test = y[mask_tr] , y[~mask_tr]\n",
    "            self.test_index_mask = (~mask_tr).values\n",
    "            print(f\"Time-based split with cutoff {cutoff_date}: train={mask_tr.sum():,} test={(~mask_tr).sum():,}\")\n",
    "        else:\n",
    "            X_train , X_test , y_train , y_test = train_test_split(\n",
    "                  X , y\n",
    "                , test_size = test_size\n",
    "                , stratify  = y\n",
    "                , random_state = random_state\n",
    "            )\n",
    "            # store test mask (approximate) for plotting convenience\n",
    "            self.test_index_mask = np.zeros(len(self.df), dtype = bool)\n",
    "            self.test_index_mask[self.test_index_mask.shape[0] - len(y_test):] = True\n",
    "\n",
    "        # scale only for LR\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled  = self.scaler.transform(X_test)\n",
    "\n",
    "        # calibrated RF (unscaled)\n",
    "        self.rf_model = CalibratedClassifierCV(\n",
    "              RandomForestClassifier(\n",
    "                    n_estimators = 300\n",
    "                  , max_depth = 12\n",
    "                  , min_samples_leaf = 10\n",
    "                  , class_weight = 'balanced'\n",
    "                  , n_jobs = -1\n",
    "                  , random_state = random_state\n",
    "                )\n",
    "            , method = 'isotonic'\n",
    "            , cv = 3\n",
    "        )\n",
    "        self.rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # auxiliary RF for importances (faster than permutation on large data)\n",
    "        self.rf_aux_for_importance = RandomForestClassifier(\n",
    "              n_estimators = 300\n",
    "            , max_depth = 12\n",
    "            , min_samples_leaf = 10\n",
    "            , class_weight = 'balanced'\n",
    "            , n_jobs = -1\n",
    "            , random_state = random_state\n",
    "        ).fit(X_train, y_train)\n",
    "        self.feature_importance = pd.DataFrame(\n",
    "              dict(\n",
    "                    feature = X_train.columns\n",
    "                  , rf_importance = self.rf_aux_for_importance.feature_importances_\n",
    "              )\n",
    "        ).sort_values('rf_importance', ascending = False)\n",
    "\n",
    "        # calibrated LR (scaled)\n",
    "        self.lr_model = CalibratedClassifierCV(\n",
    "              LogisticRegression(\n",
    "                    penalty = 'l2'\n",
    "                  , C = 1.0\n",
    "                  , class_weight = 'balanced'\n",
    "                  , max_iter = 500\n",
    "                  , solver = 'lbfgs'\n",
    "                  , n_jobs = None\n",
    "                  , random_state = random_state\n",
    "                )\n",
    "            , method = 'isotonic'\n",
    "            , cv = 3\n",
    "        )\n",
    "        self.lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # evaluation\n",
    "        rf_p = self.rf_model.predict_proba(X_test)[:, 1]\n",
    "        lr_p = self.lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        ens  = 0.6 * rf_p + 0.4 * lr_p\n",
    "\n",
    "        roc  = float(roc_auc_score(y_test, ens))\n",
    "        pr   = float(average_precision_score(y_test, ens))\n",
    "        lift , base = self.lift_table(ens, y_test, bins = 10)\n",
    "\n",
    "        self.eval_metrics = dict(\n",
    "              roc_auc = roc\n",
    "            , pr_auc  = pr\n",
    "            , base_rate = float(base)\n",
    "            , n_train = int(len(y_train))\n",
    "            , n_test  = int(len(y_test))\n",
    "        )\n",
    "        self.lift_df = lift\n",
    "\n",
    "        print(f\"AUC-ROC: {roc:.3f} | PR-AUC: {pr:.3f} | Base rate: {base:.3f}\")\n",
    "        return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # create a faster similarity funciton for when weight is set to zero \n",
    "    def similarity_to_subscribers_fast(\n",
    "      self\n",
    "    , n_estimators = 200\n",
    "    , max_depth    = 16\n",
    "    , random_state = 42\n",
    "  ):\n",
    "      # fast unsupervised leaf-sharing similarity to current subscribers\n",
    "      X = self.features.values.astype(np.float32)\n",
    "      n , m = X.shape\n",
    "      rng = np.random.default_rng(random_state)\n",
    "\n",
    "      # column-wise permutation synthetic class\n",
    "      X_synth = np.vstack([rng.permutation(X[:, j]) for j in range(m)]).T\n",
    "      X_all   = np.vstack([X, X_synth])\n",
    "      y_all   = np.concatenate([np.ones(n, np.int8), np.zeros(n, np.int8)])\n",
    "\n",
    "      rf = RandomForestClassifier(\n",
    "            n_estimators   = n_estimators\n",
    "          , max_depth      = max_depth\n",
    "          , min_samples_leaf = 5\n",
    "          , max_features   = 'sqrt'\n",
    "          , bootstrap      = True\n",
    "          , max_samples    = 0.6\n",
    "          , n_jobs         = -1\n",
    "          , random_state   = random_state\n",
    "      ).fit(X_all, y_all)\n",
    "\n",
    "      leaves   = rf.apply(X)  # (n, trees)\n",
    "      sub_mask = self.df['is_subscriber_now'].astype(int).values == 1\n",
    "      if sub_mask.sum() == 0:\n",
    "          return np.zeros(n, dtype = float)\n",
    "\n",
    "      accum = np.zeros(n, dtype = float)\n",
    "      for t in range(leaves.shape[1]):\n",
    "          lt = leaves[:, t]\n",
    "          uniq , inv = np.unique(lt, return_inverse = True)\n",
    "          counts = np.bincount(inv[sub_mask], minlength = uniq.size)\n",
    "          accum += counts[inv]\n",
    "      return accum / (leaves.shape[1] * sub_mask.sum())\n",
    "\n",
    "\n",
    "    # rank our results \n",
    "    def identify_lookalikes(\n",
    "        self\n",
    "      , top_n = 1000\n",
    "      , sim_weight = 0.0   # set a weight to blend our models, 0 means pure propensity score weight, 1 means pure similarity; anything between 0.1–0.3 will pull prospects toward dense subscriber neighborhoods\n",
    "    ):\n",
    "        print(\"Identifying subscriber lookalikes...\")\n",
    "\n",
    "        # propensity everywhere for ranking later\n",
    "        rf_probs = self.rf_model.predict_proba(self.features)[:, 1]\n",
    "        lr_probs = self.lr_model.predict_proba(self.scaler.transform(self.features))[:, 1]\n",
    "        ensemble = 0.6 * rf_probs + 0.4 * lr_probs\n",
    "\n",
    "        if sim_weight > 0.0:\n",
    "            sim = self.similarity_to_subscribers_fast()\n",
    "            ensemble = (1 - sim_weight) * ensemble + sim_weight * sim\n",
    "\n",
    "        self.ensemble_probs_all = ensemble\n",
    "\n",
    "        results = pd.DataFrame(\n",
    "              dict(\n",
    "                    user_id = self.df['user_id'].values\n",
    "                  , is_subscriber = self.df['is_subscriber_now'].astype(int).values\n",
    "                  , rf_propensity = rf_probs\n",
    "                  , lr_propensity = lr_probs\n",
    "                  , ensemble_propensity = ensemble\n",
    "                  , is_active = self.df['is_active'].values\n",
    "                  , days_since_created = self.df['days_since_created'].values\n",
    "                  , n_teams = self.df['n_teams'].values\n",
    "                  , is_engaged = self.df['is_engaged'].values\n",
    "              )\n",
    "        )\n",
    "\n",
    "        non_subs = results[results['is_subscriber'] == 0].copy()\n",
    "        non_subs['propensity_rank'] = non_subs['ensemble_propensity'].rank(ascending = False, method = 'first')\n",
    "\n",
    "        quality = non_subs[\n",
    "              (non_subs['is_active'] == 1)\n",
    "            & (non_subs['days_since_created'] >= 7)\n",
    "            & (non_subs['n_teams'] > 0)\n",
    "        ].copy()\n",
    "\n",
    "        top_prospects = quality.nsmallest(top_n, 'propensity_rank')\n",
    "        print(f\"Identified {len(top_prospects)} high-quality prospects | Avg prop: {top_prospects['ensemble_propensity'].mean():.3f}\")\n",
    "        return top_prospects , results\n",
    "\n",
    "    # create an insights data frame to summarize key findings for top figures\n",
    "    def generate_insights(self, top_prospects):\n",
    "        print(\"\\nGenerating insights...\")\n",
    "        subscribers = self.df[self.df['is_subscriber_now'] == 1]\n",
    "        metrics = [\n",
    "            'n_current_teams','is_engaged','is_super_engaged'\n",
    "          , 'n_livestreams_viewed','account_age_weeks','total_content_viewed'\n",
    "        ]\n",
    "        insights = {}\n",
    "        ids = set(top_prospects['user_id'])\n",
    "        for m in metrics:\n",
    "            if m in self.df.columns:\n",
    "                prospect_avg  = self.df[self.df['user_id'].isin(ids)][m].mean()\n",
    "                subscriber_avg = subscribers[m].mean()\n",
    "                insights[m] = dict(\n",
    "                      prospect_avg    = float(prospect_avg)\n",
    "                    , subscriber_avg  = float(subscriber_avg)\n",
    "                    , similarity_ratio = float(prospect_avg / (subscriber_avg + 1e-9))\n",
    "                )\n",
    "        return insights\n",
    "\n",
    "    # adding this in to avoid having to tinker later on, but let's viz some of our output\n",
    "    def plot_results(self, results, top_prospects):\n",
    "        fig , axes = plt.subplots(2, 2, figsize = (15, 12))\n",
    "\n",
    "        # propensity distributions\n",
    "        ns_mask = results['is_subscriber'] == 0\n",
    "        s_mask  = ~ns_mask\n",
    "        axes[0,0].hist(results.loc[ns_mask, 'ensemble_propensity'], bins = 50, alpha = 0.7, density = True, label = 'Non-subs')\n",
    "        axes[0,0].hist(results.loc[s_mask,  'ensemble_propensity'], bins = 50, alpha = 0.7, density = True, label = 'Subs')\n",
    "        axes[0,0].axvline(top_prospects['ensemble_propensity'].min(), linestyle = '--', label = 'Prospect threshold')\n",
    "        axes[0,0].set_title('Propensity Distribution'); axes[0,0].legend()\n",
    "\n",
    "        # top RF features\n",
    "        topF = self.feature_importance.head(15)\n",
    "        axes[0,1].barh(range(len(topF)), topF['rf_importance'].values)\n",
    "        axes[0,1].set_yticks(range(len(topF))); axes[0,1].set_yticklabels(topF['feature'].values)\n",
    "        axes[0,1].invert_yaxis(); axes[0,1].set_title('Top Feature Importances (RF)')\n",
    "\n",
    "        # propsects by competition level\n",
    "        if 'primary_competition_daily' in self.df.columns:\n",
    "            # latest row per user to avoid duplicates\n",
    "            tmp = (\n",
    "                self.df.sort_values('as_of_date')\n",
    "                      .groupby('user_id', as_index = False)\n",
    "                      .tail(1)\n",
    "            )\n",
    "            prospect_ids = set(top_prospects['user_id'])\n",
    "            p = tmp[tmp['user_id'].isin(prospect_ids)].copy()\n",
    "\n",
    "            comp_counts = (\n",
    "                p['primary_competition_daily']\n",
    "                  .fillna('unknown')\n",
    "                  .value_counts()\n",
    "                  .sort_values(ascending = False)\n",
    "            )\n",
    "            axes[1,0].bar(comp_counts.index, comp_counts.values)\n",
    "            axes[1,0].set_xlabel('Competition Level')\n",
    "            axes[1,0].set_ylabel('Top Prospect Count')\n",
    "            axes[1,0].set_title('Top Prospects by Competition Level')\n",
    "            axes[1,0].tick_params(axis = 'x', rotation = 20)\n",
    "\n",
    "            # (optional) stacked by age group \n",
    "            # if 'self_age_group_condensed' in p.columns:\n",
    "            #     comp_age = (p.assign(\n",
    "            #                   comp = p['primary_competition_daily'].fillna('unknown')\n",
    "            #                 , age  = p['self_age_group_condensed'].fillna('unknown'))\n",
    "            #                 .groupby(['comp','age']).size().unstack(fill_value = 0)\n",
    "            #               )\n",
    "            #     axes[1,0].clear()\n",
    "            #     bottom = np.zeros(len(comp_age))\n",
    "            #     for age in comp_age.columns:\n",
    "            #         axes[1,0].bar(comp_age.index, comp_age[age].values, bottom = bottom, label = str(age))\n",
    "            #         bottom += comp_age[age].values\n",
    "            #     axes[1,0].set_xlabel('Competition Level')\n",
    "            #     axes[1,0].set_ylabel('Top Prospect Count')\n",
    "            #     axes[1,0].set_title('Top Prospects by Competition Level (stacked by Age Group)')\n",
    "            #     axes[1,0].legend(fontsize = 8)\n",
    "        else:\n",
    "            axes[1,0].text(0.5, 0.5, 'primary_competition_daily not found', ha = 'center')\n",
    "\n",
    "        # ROC for the nerds (I'm the nerds)\n",
    "        if self.test_index_mask is not None and self.ensemble_probs_all is not None:\n",
    "            test_mask = self.test_index_mask\n",
    "            y_test_all = self.df['is_subscriber_now'].astype(int).values[test_mask]\n",
    "            s_test_all = self.ensemble_probs_all[test_mask]\n",
    "            if y_test_all.size > 0:\n",
    "                from sklearn.metrics import roc_curve\n",
    "                fpr , tpr , _ = roc_curve(y_test_all, s_test_all)\n",
    "                axes[1,1].plot(fpr, tpr, label = 'Ensemble ROC')\n",
    "                axes[1,1].plot([0,1],[0,1],'k--', alpha = 0.5)\n",
    "                axes[1,1].set_title('ROC Curve'); axes[1,1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# running function\n",
    "def run_subscriber_lookalike_analysis(\n",
    "    query_2\n",
    "  , top_n = 1000\n",
    "  , cutoff_date = None     # e.g., '2025-06-30' for time-based split\n",
    "  , sim_weight = 0.0       # set to e.g. 0.2 to blend in similarity\n",
    "):\n",
    "    analysis = SubscriberLookalikeAnalysis(query_2)\n",
    "\n",
    "    features , target = analysis.prepare_features()\n",
    "    X_train , X_test , y_train , y_test = analysis.train_propensity_models(\n",
    "          test_size = 0.2\n",
    "        , random_state = 42\n",
    "        , cutoff_date = cutoff_date\n",
    "    )\n",
    "\n",
    "    top_prospects , results = analysis.identify_lookalikes(\n",
    "          top_n = top_n\n",
    "        , sim_weight = sim_weight\n",
    "    )\n",
    "    insights = analysis.generate_insights(top_prospects)\n",
    "    fig = analysis.plot_results(results, top_prospects)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUBSCRIBER LOOKALIKE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total users analyzed: {len(query_2):,}\")\n",
    "    print(f\"Current subscribers: {int(target.sum()):,} ({target.mean()*100:.1f}%)\")\n",
    "    print(f\"High-quality prospects identified: {len(top_prospects):,}\")\n",
    "    print(f\"Average prospect propensity: {top_prospects['ensemble_propensity'].mean():.3f}\")\n",
    "    print(f\"Top prospect propensity: {top_prospects['ensemble_propensity'].max():.3f}\")\n",
    "    print(\"\\nEval:\", analysis.eval_metrics)\n",
    "    print(\"\\nLift table (deciles):\")\n",
    "    print(analysis.lift_df.to_string(index = True))\n",
    "\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    print(analysis.feature_importance.head(10)[['feature','rf_importance']].to_string(index = False))\n",
    "\n",
    "    return top_prospects , analysis , insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering...\n",
      "Feature engineering complete. Using 90 features.\n",
      "Training propensity models (calibrated)...\n"
     ]
    }
   ],
   "source": [
    "# normalize column names\n",
    "query_2 = query_2.copy()\n",
    "query_2.columns = [str(c).lower() for c in query_2.columns]\n",
    "\n",
    "# run the analysis\n",
    "\n",
    "top_prospects , analysis , insights = run_subscriber_lookalike_analysis(\n",
    "    query_2\n",
    "  , top_n = 500000\n",
    "  , cutoff_date = None   \n",
    "  , sim_weight = 0.5\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a1c82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n",
      "\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# run it\u001b[39;00m\n",
      "\u001b[0;32m---> 44\u001b[0m insights_df_latest \u001b[38;5;241m=\u001b[39m insights_per_user_latest(\u001b[43manalysis\u001b[49m, top_prospects)\n",
      "\u001b[1;32m     45\u001b[0m insights_df_latest\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analysis' is not defined"
     ]
    }
   ],
   "source": [
    "def insights_per_user_latest(\n",
    "    analysis\n",
    "  , top_prospects\n",
    "  , metrics = (\n",
    "        'days_since_last_active'\n",
    "      , 'n_current_teams'\n",
    "      , 'is_engaged'\n",
    "      , 'is_super_engaged'\n",
    "      , 'n_livestreams_viewed'\n",
    "      , 'days_since_created'\n",
    "      , 'total_content_viewed'\n",
    "    )\n",
    "):\n",
    "    tmp = analysis.df.assign(\n",
    "        propensity = analysis.ensemble_probs_all\n",
    "    )\n",
    "    latest = (\n",
    "        tmp.sort_values('as_of_date')\n",
    "           .groupby('user_id', as_index = False)\n",
    "           .tail(1)\n",
    "    )\n",
    "\n",
    "    prospect_ids = set(top_prospects['user_id'])\n",
    "    prospects     = latest[latest['user_id'].isin(prospect_ids)]\n",
    "    subscribers   = latest[latest['is_subscriber_now'].astype(int) == 1]\n",
    "\n",
    "    rows = []\n",
    "    for m in metrics:\n",
    "        if m in latest.columns:\n",
    "            p_avg = prospects[m].mean()\n",
    "            s_avg = subscribers[m].mean()\n",
    "            rows.append(\n",
    "                dict(\n",
    "                    metric = m\n",
    "                  , prospect_avg = float(p_avg)\n",
    "                  , subscriber_avg = float(s_avg)\n",
    "                  , similarity_ratio = float(p_avg / (s_avg + 1e-9))\n",
    "                  , diff = float(p_avg - s_avg)\n",
    "                )\n",
    "            )\n",
    "    return pd.DataFrame(rows).sort_values('similarity_ratio', ascending = False)\n",
    "\n",
    "# run it\n",
    "insights_df_latest = insights_per_user_latest(analysis, top_prospects)\n",
    "insights_df_latest\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
